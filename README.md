# EMNIST_ML
Распознавание латинских букв с картинок классическими ML-алгоритмами
## Задача - распознать на изображении рукописную латинскую букву, используя исключительно классические ML-алгоритмы

### Источники данных
Для обучения использован тренировочный датасет emnist-letters-train.csv и тестовый датасет emnist-letters-test.csv с https://www.kaggle.com/datasets/crawford/emnist/data
В датасете 785 колонок: столбец 23 - номер буквы (в латинском алфавите всего 26 букв), остальные колонки - это цвет пикселя (28*28 = 784).
Стоит обратить внимание, что тренировочная и тестовая выборки содержат буквы, повернутые на 90 градусов влево.

###  Используемые методы
Требование задачи состоит в том, чтобы использовать классические ML-алгоритмы и выбрать наилучшее решение.
1. Для решения задачи многоклассовой классификации были использованы следующие алгоритмы:
- KNeighborsClassifier,
- LogisticRegression,
- Наивный Байес (MultinomialNB, ComplementNB, BernoulliNB),
- SVC (LinearSVC, SVC с ядром poly и ядром rbf),
- DecisionTreeClassifier.
2. Для бенчмарка были рассмотрены ансамбли GradientBoostingClassifier и RandomForestClassifier.
3. Для подбора параметров ML-алгоритмов использованы:
  - функция evaluate, устанавливающая параметры через set_params(**params),
  - GridSearchCV

### Метрики
Для оценки качества классификации каждого алгоритма используется Accuracy. EDA показал, что классы сбалансированы в тренировочной выборке.
Для того, чтобы оценить, как хорошо предсказывается каждый отдельный класс использован classification_report.

### Суть решения
1. Каждый столбец кроме 1-го содержит цвет пикселя. Цвет пикселя имеет 255 значений. Чтобы не было слишком большого разброса значений признаков, значения были нормализованы.
2. Были получены следующие значения accuracy.

![image](https://github.com/lili-alsh/EMNIST_ML/assets/54451801/1190c167-92b3-45c3-a105-d647728b7b6c)

4. Для повышения качества и производительности алгоритмов необходимо уменьшить количество признаков. Чтобы оставить наиболее информативные пиксели был применен аналог пуллинга с окном 2*2, в итоге осталось 729 признаков. Для последующего понижения размерности использован PCA, благодаря которому остался 51 признак.
5. Осуществлялся подбор гиперпараметров для классических ML-алгоритмов:
   - для LogisticRegression подбирался параметр C, tol, solver, max_iter. Наибольшее значение accuracy достигается при С=0.866 и остальных параметрах по умолчанию.
   - для BernoulliNB подбирался параметр alpha. Наибольший accuracy достигается при показателе alpha=0.
   - среди алгоритмов SVC наилучшее значение по умолчанию показали нелинейные SVC с ядром poly и rbf. Для нелинейных SVC подбирались параметры C, gamma и kernel. Наибольший accuracy достигается при параметрах C= 7, gamma по умолчанию, kernel = rbf.
   - для DecisionTreeClassifier подбирался параметр max_depth.
7. В итоге были получены следующие значения accuracy.

   ![image](https://github.com/lili-alsh/EMNIST_ML/assets/54451801/5cc8b992-9aa1-4279-b4fd-05e3b2c1d467)

Как и в случае с данными без понижения размерности наилучший результат accuracy показал SVC, причем самый лучший результат также был достигнут с rbf ядром. За счет PCA удалось повысить качество на 2%, причем время обучения модели сократилось почти на 60%. Отчет classification_report показывает, что в разрезе классов SVC с ядром rbf также показывает результаты получше.

Стоит отметить, что ансамбли GradientBoostingClassifier и RandomForestClassifier дали 0,75 и 0,86 соответственно. 
